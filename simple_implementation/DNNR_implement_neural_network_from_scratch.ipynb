{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iV2SLutKtgea"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_regression\n",
        "import pandas as pd\n",
        "import math\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_squared_error "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some ideas and the usage are borrowed from deeplearning.ai"
      ],
      "metadata": {
        "id": "8c62PyTDOKRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "class DNNR:\n",
        "\n",
        "  def __init__(self, learning_rate = 0.01, epochs = 3000, layers_dims = [13, 10, 5, 1], layers_activation = ['relu','relu','leaky_relu'], weight_init = 'Xavier',\n",
        "               keep_prob = 0.9, regularization = 'l2', lam = 0.05, task = \"train\", batch = None, beta1 = 0.99, beta2 = 0.999, epsilon = 1e-8, solver='sgd', \n",
        "               decay = False, decay_rate = 0.0001, interval = 1000, verbose = 0):\n",
        "    \n",
        "    # Some simple judgements according to the setup\n",
        "    self.error_msg(layers_dims, layers_activation, weight_init, keep_prob, regularization, solver)\n",
        "    self.learning_rate = learning_rate\n",
        "    self.epochs = epochs\n",
        "    self.keep_prob = keep_prob\n",
        "    self.regularization = regularization\n",
        "    self.lam = lam\n",
        "    self.weight_init = weight_init\n",
        "    self.Activation = layers_activation\n",
        "    self.layers_dims = layers_dims\n",
        "    self.task = task\n",
        "    self.batch = batch\n",
        "    self.solver = solver\n",
        "    self.beta1 = beta1\n",
        "    self.beta2 = beta2\n",
        "    self.t = 2\n",
        "    self.epsilon = epsilon\n",
        "    self.decay = decay\n",
        "    self.decay_rate = decay_rate\n",
        "    self.interval = interval\n",
        "    self.verbose = verbose\n",
        "    self.W = None\n",
        "    self.b = None\n",
        "    self.grads = None\n",
        "    self.dA = None\n",
        "    self.dW = None\n",
        "    self.db = None\n",
        "    self.caches = None\n",
        "    self.L = None\n",
        "    self.D = None\n",
        "    self.v_dw = None\n",
        "    self.v_db = None\n",
        "    self.s_dw = None\n",
        "    self.s_db = None\n",
        "    self.initialize(self.layers_dims, self.Activation)\n",
        "    \n",
        "  \n",
        "  def error_msg(self, layers_dims, layers_activation, weight_init, keep_prob, regularization, solver):\n",
        "\n",
        "    # whether the layers_dims shape and layers_activation shape is correct\n",
        "    assert len(layers_dims) == len(layers_activation) + 1, \"Wrong lengths of layers_dims and layers_activation:(\"\n",
        "\n",
        "    # whether the activation functions are valid\n",
        "    activation_test = True\n",
        "    for activation in layers_activation: \n",
        "      if activation not in ['relu','sigmoid','tanh','identity','leaky_relu']: activation_test = False\n",
        "    assert activation_test == True, \"Invalid activation function:(\"\n",
        "\n",
        "    # whether the weight_init method is valid\n",
        "    weight_init_test = True\n",
        "    if weight_init not in ['Xavier','He']: weight_init_test = False\n",
        "    assert weight_init_test == True, \"Invalid weight_init method:(\"\n",
        "\n",
        "    assert 0 < keep_prob <= 1.0, \"keep_prob should be between 0 and 1:(\"\n",
        "\n",
        "    assert regularization in [None, 'l2'], \"Invalid regularization method:(\"\n",
        "\n",
        "    assert solver in ['sgd','adam'], \"Invalid solver:(\"\n",
        "\n",
        "  \n",
        "  def initialize(self, layers_dims, layers_activation):\n",
        "    \n",
        "    # initialize the parameters for the given settings\n",
        "    L = len(layers_dims)\n",
        "    if self.weight_init == 'Xavier':\n",
        "      self.W = [np.random.randn(layers_dims[l], layers_dims[l - 1]) / np.sqrt(layers_dims[l - 1]) for l in range(1, L)]\n",
        "    elif self.weight_init == 'He':\n",
        "      self.W = [np.random.randn(layers_dims[l], layers_dims[l - 1]) / np.sqrt(2/layers_dims[l - 1]) for l in range(1, L)]\n",
        "    self.L = len(self.W)\n",
        "    # Shape initialization\n",
        "    # Adam\n",
        "    self.v_dw = [np.zeros((layers_dims[l], layers_dims[l - 1])) for l in range(1, L)]\n",
        "    self.v_db = [np.zeros((layers_dims[l], 1)) for l in range(1, L)]\n",
        "    self.s_dw = [np.zeros((layers_dims[l], layers_dims[l - 1])) for l in range(1, L)]\n",
        "    self.s_db = [np.zeros((layers_dims[l], 1)) for l in range(1, L)]\n",
        "    # Wight and bias\n",
        "    self.b = [np.zeros((layers_dims[l], 1)) for l in range(1, L)]\n",
        "    self.Activation = layers_activation\n",
        "    self.caches = [[] for _ in range(L - 1)]\n",
        "    self.dA = [[] for _ in range(L)]\n",
        "    self.dW = [[] for _ in range(L - 1)]\n",
        "    self.db = [[] for _ in range(L - 1)]\n",
        "    # Dropout\n",
        "    self.D = [[] for _ in range(L - 2)]\n",
        "\n",
        "    # For regularization: if there's no regularization, set lambda to 0\n",
        "    if self.regularization == None: self.lam = 0\n",
        "\n",
        "  def forward(self, X):\n",
        "\n",
        "    A = X\n",
        "\n",
        "    # for the forward propogation, make the predictions\n",
        "    for l in range(self.L):\n",
        "\n",
        "        A_prev = A \n",
        "        W = self.W[l]\n",
        "        b = self.b[l]\n",
        "        activation = self.Activation[l]\n",
        "\n",
        "        # the linear part\n",
        "        Z = np.dot(W, A_prev) + b\n",
        "\n",
        "        # a = activation(z)\n",
        "        if activation == \"sigmoid\":   \n",
        "          A = 1/(1 + np.exp(-Z))\n",
        "        elif activation == \"relu\": \n",
        "          A = np.maximum(0, Z)\n",
        "        elif activation == \"identity\":\n",
        "          A = Z\n",
        "        elif activation == \"tanh\":\n",
        "          A = np.tanh(Z)\n",
        "        elif activation == \"leaky_relu\":\n",
        "          A = np.fmax(0.01*Z, Z)\n",
        "        \n",
        "        # Implement Dropout in training models (not in computing MSE)\n",
        "        if l != self.L - 1 and self.task == \"train\":\n",
        "          A = (A * self.D[l]) / self.keep_prob\n",
        "  \n",
        "        # keep those in caches --> don't need to recompute it during back propogation\n",
        "        self.caches[l] = [A_prev, Z]\n",
        "\n",
        "        if l == self.L - 1:\n",
        "          Y_predict = A\n",
        "\n",
        "    return Y_predict\n",
        "  \n",
        " \n",
        "  def backward(self, Y_predict, Y):\n",
        "  \n",
        "    m = Y_predict.shape[1]\n",
        "\n",
        "    # initialize the back prop\n",
        "    self.dA[-1] = (Y_predict - Y)\n",
        "    \n",
        "    for l in range(self.L - 1, -1, -1):\n",
        "        \n",
        "        dA = self.dA[l + 1]\n",
        "\n",
        "        A_prev = self.caches[l][0]\n",
        "        Z = self.caches[l][1]\n",
        "        W = self.W[l]\n",
        "        b = self.b[l]\n",
        "        activation = self.Activation[l]\n",
        "\n",
        "        if activation == \"relu\":   \n",
        "          dA[Z <= 0] = 0\n",
        "          dZ = dA\n",
        "        elif activation == \"sigmoid\":\n",
        "          dZ = dA * (1/(1 + np.exp(-Z))) * (1 - (1/(1 + np.exp(-Z))))     \n",
        "        elif activation == \"tanh\":\n",
        "          dZ = dA * ((1 - np.tanh(Z)) ** 2)\n",
        "        elif activation == \"identity\":\n",
        "          dZ = dA\n",
        "        elif activation == \"leaky_relu\":\n",
        "          dA[Z <= 0] = 0.01\n",
        "          dZ = dA\n",
        "\n",
        "        # regularization is implemented\n",
        "        self.dW[l] = 1/m * np.dot(dZ, A_prev.T) + (self.lam / m) * W\n",
        "        self.db[l] = 1/m * np.sum(dZ, axis = 1, keepdims = True)\n",
        "        self.dA[l] = np.dot(W.T, dZ)\n",
        "        \n",
        "        # Dropout\n",
        "        if l != 0:\n",
        "          self.dA[l] = (self.dA[l] * self.D[l - 1]) / self.keep_prob\n",
        "        \n",
        "    \n",
        "  def update_sgd(self):\n",
        "    \n",
        "    if self.decay: self.learning_rate *= 1 / (1 + self.decay_rate * math.floor(self.epochs / self.interval))\n",
        "\n",
        "    for l in range(self.L):\n",
        "        self.W[l] = self.W[l] - self.learning_rate * self.dW[l]\n",
        "        self.b[l] = self.b[l] - self.learning_rate * self.db[l]\n",
        "  \n",
        "  def update_adam(self):\n",
        "\n",
        "    if self.decay: self.learning_rate *= 1 / (1 + self.decay_rate * math.floor(self.epochs / self.interval))\n",
        "\n",
        "    for l in range(self.L):\n",
        "      # The corrected version of v and s -- a combination of momentum and rmsprop\n",
        "      self.v_dw[l] = self.beta1 * self.v_dw[l] + (1 - self.beta1) * self.dW[l]\n",
        "      self.v_db[l] = self.beta1 * self.v_db[l] + (1 - self.beta1) * self.db[l]\n",
        "      v_dw_c = self.v_dw[l] / (1 - self.beta1 ** self.t)\n",
        "      v_db_c = self.v_db[l] / (1 - self.beta1 ** self.t)\n",
        "      self.s_dw[l] = self.beta2 * self.s_dw[l] + (1 - self.beta2) * np.square(self.dW[l])\n",
        "      self.s_db[l] = self.beta2 * self.s_db[l] + (1 - self.beta2) * np.square(self.db[l])\n",
        "      s_dw_c = self.s_dw[l] / (1 - self.beta2 ** self.t)\n",
        "      s_db_c = self.s_db[l] / (1 - self.beta2 ** self.t)\n",
        "\n",
        "      # Update w and b\n",
        "      self.W[l] = self.W[l] - self.learning_rate * v_dw_c / (np.sqrt(s_dw_c) + self.epsilon)\n",
        "      self.b[l] = self.b[l] - self.learning_rate * v_db_c / (np.sqrt(s_db_c) + self.epsilon)\n",
        "\n",
        "\n",
        "  def dropout(self, m):\n",
        "\n",
        "    for hidden_layer in range(self.L - 1):\n",
        "      self.D[hidden_layer] = np.random.rand(self.layers_dims[hidden_layer + 1], m) < self.keep_prob\n",
        "  \n",
        "  def fit(self, X, Y):\n",
        "\n",
        "    X = np.array(X).T\n",
        "    Y = np.array(Y).reshape(1, -1)\n",
        "\n",
        "    m = Y.shape[1]\n",
        "\n",
        "    if self.batch == None: self.batch = m\n",
        "    # Random pick the mini_batches\n",
        "    mini_batches = []\n",
        "    permutation = list(np.random.permutation(m))\n",
        "    shuffled_X = X[:, permutation]\n",
        "    shuffled_Y = Y[:, permutation].reshape((1, m))\n",
        "    count = math.floor(m / self.batch)\n",
        "    for k in range(count):\n",
        "        mini_batches.append((shuffled_X[:, k*self.batch : (k+1)*self.batch],shuffled_Y[:, k*self.batch: (k+1)*self.batch]))\n",
        "    if m % self.batch != 0:\n",
        "        mini_batches.append((shuffled_X[:, int(m/self.batch)*self.batch :],shuffled_Y[:, int(m/self.batch)*self.batch : ]))\n",
        "\n",
        "    for i in range(self.epochs):\n",
        "      for mini_batch in mini_batches:\n",
        "        (minibatch_X, minibatch_Y) = mini_batch\n",
        "        self.dropout(minibatch_Y.shape[1])\n",
        "        Y_predict = self.forward(minibatch_X)\n",
        "        self.backward(Y_predict, minibatch_Y)\n",
        "\n",
        "        # sgd and adam update methods  \n",
        "        if self.solver == 'sgd':\n",
        "          self.update_sgd()\n",
        "        elif self.solver == 'adam':\n",
        "          self.t += 1\n",
        "          self.update_adam()\n",
        "  \n",
        "  def MSE(self, X, Y):\n",
        "\n",
        "    # Do not use dropout during the calculation of error\n",
        "    self.task = 'test'\n",
        "\n",
        "    X = np.array(X).T\n",
        "    Y = np.array(Y).reshape(1, -1)\n",
        "    Y_predict = self.forward(X)\n",
        "\n",
        "    self.task = 'train'\n",
        "\n",
        "    return np.mean((Y_predict - Y) ** 2)  \n",
        "  "
      ],
      "metadata": {
        "id": "wxLLHv-vh9uM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "boston = pd.read_csv(\"http://43.143.180.76/data/boston.csv\").sample(frac=1.0).reset_index(drop=True)\n",
        "n = len(boston) * 0.7\n",
        "columns = list(boston.columns)\n",
        "\n",
        "X_train = np.array(boston.loc[:n][columns[:-1]])\n",
        "Y_train = np.array(boston.loc[:n][columns[-1]])\n",
        "X_test = np.array(boston.loc[n:][columns[:-1]])\n",
        "Y_test = np.array(boston.loc[n:][columns[-1]])\n",
        "\n",
        "print(\"X.shape:\", X_train.shape)\n",
        "print(\"Y.shape\", Y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_N0yN9jlfDPs",
        "outputId": "35c368b4-8fd6-4658-8975-cd7c1b509f6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X.shape: (355, 13)\n",
            "Y.shape (355,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = DNNR()\n",
        "model.fit(X_train,Y_train)\n",
        "mse_train = model.MSE(X_train,Y_train)\n",
        "mse_test = model.MSE(X_test,Y_test)\n",
        "print(mse_train)\n",
        "print(mse_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MYdPiKEfQj8",
        "outputId": "52eb4e1a-e7be-44ba-d3b3-f0c450b22278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.522385101882724\n",
            "8.71071611879156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MLP = MLPRegressor(max_iter = 3000, learning_rate_init=0.01)\n",
        "MLP = MLP.fit(X_train,Y_train)\n",
        "MLP_Y_train_predict = MLP.predict(X_train)\n",
        "MLP_Y_test_predict = MLP.predict(X_test)\n",
        "mse_mlp_train = mean_squared_error(Y_train,MLP_Y_train_predict)\n",
        "mse_mlp_test = mean_squared_error(Y_test,MLP_Y_test_predict)\n",
        "print(mse_mlp_train)\n",
        "print(mse_mlp_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDxsTYmxJWJn",
        "outputId": "2c7b684f-0dfb-49df-d947-c4ca3266b7b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.994014705859778\n",
            "8.861343306833792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Compared DNN with MLP, the results are kind of similar if I run the result multiple times. Both MSE are around 10."
      ],
      "metadata": {
        "id": "IP2TXw9Hg3s7"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
