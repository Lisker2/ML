{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iV2SLutKtgea"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_regression\n",
        "import pandas as pd\n",
        "import math\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_squared_error "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some ideas and the usage are borrowed from deeplearning.ai"
      ],
      "metadata": {
        "id": "8c62PyTDOKRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "class DNNR:\n",
        "\n",
        "  def __init__(self, learning_rate = 0.01, epochs = 3000, layers_dims = [13, 10, 5, 1], layers_activation = ['relu','relu','leaky_relu'], weight_init = 'Xavier',\n",
        "               keep_prob = 0.9, regularization = 'l2', lam = 0.05, task = \"train\", batch = None, beta1 = 0.99, beta2 = 0.999, epsilon = 1e-8, solver='sgd', \n",
        "               decay = False, decay_rate = 0.0001, interval = 1000, verbose = 0):\n",
        "    \n",
        "    # Some simple judgements according to the setup\n",
        "    self.error_msg(layers_dims, layers_activation, weight_init, keep_prob, regularization, solver)\n",
        "    self.learning_rate = learning_rate\n",
        "    self.epochs = epochs\n",
        "    self.keep_prob = keep_prob\n",
        "    self.regularization = regularization\n",
        "    self.lam = lam\n",
        "    self.weight_init = weight_init\n",
        "    self.Activation = layers_activation\n",
        "    self.layers_dims = layers_dims\n",
        "    self.task = task\n",
        "    self.batch = batch\n",
        "    self.solver = solver\n",
        "    self.beta1 = beta1\n",
        "    self.beta2 = beta2\n",
        "    self.t = 2\n",
        "    self.epsilon = epsilon\n",
        "    self.decay = decay\n",
        "    self.decay_rate = decay_rate\n",
        "    self.interval = interval\n",
        "    self.verbose = verbose\n",
        "    self.W = None\n",
        "    self.b = None\n",
        "    self.grads = None\n",
        "    self.dA = None\n",
        "    self.dW = None\n",
        "    self.db = None\n",
        "    self.caches = None\n",
        "    self.L = None\n",
        "    self.D = None\n",
        "    self.v_dw = None\n",
        "    self.v_db = None\n",
        "    self.s_dw = None\n",
        "    self.s_db = None\n",
        "    self.initialize(self.layers_dims, self.Activation)\n",
        "    \n",
        "  \n",
        "  def error_msg(self, layers_dims, layers_activation, weight_init, keep_prob, regularization, solver):\n",
        "\n",
        "    # whether the layers_dims shape and layers_activation shape is correct\n",
        "    assert len(layers_dims) == len(layers_activation) + 1, \"Wrong lengths of layers_dims and layers_activation:(\"\n",
        "\n",
        "    # whether the activation functions are valid\n",
        "    activation_test = True\n",
        "    for activation in layers_activation: \n",
        "      if activation not in ['relu','sigmoid','tanh','identity','leaky_relu']: activation_test = False\n",
        "    assert activation_test == True, \"Invalid activation function:(\"\n",
        "\n",
        "    # whether the weight_init method is valid\n",
        "    weight_init_test = True\n",
        "    if weight_init not in ['Xavier','He']: weight_init_test = False\n",
        "    assert weight_init_test == True, \"Invalid weight_init method:(\"\n",
        "\n",
        "    assert 0 < keep_prob <= 1.0, \"keep_prob should be between 0 and 1:(\"\n",
        "\n",
        "    assert regularization in [None, 'l2'], \"Invalid regularization method:(\"\n",
        "\n",
        "    assert solver in ['sgd','adam'], \"Invalid solver:(\"\n",
        "\n",
        "  \n",
        "  def initialize(self, layers_dims, layers_activation):\n",
        "    \n",
        "    # initialize the parameters for the given settings\n",
        "    L = len(layers_dims)\n",
        "    if self.weight_init == 'Xavier':\n",
        "      self.W = [np.random.randn(layers_dims[l], layers_dims[l - 1]) / np.sqrt(layers_dims[l - 1]) for l in range(1, L)]\n",
        "    elif self.weight_init == 'He':\n",
        "      self.W = [np.random.randn(layers_dims[l], layers_dims[l - 1]) / np.sqrt(2/layers_dims[l - 1]) for l in range(1, L)]\n",
        "    self.L = len(self.W)\n",
        "    # Shape initialization\n",
        "    # Adam\n",
        "    self.v_dw = [np.zeros((layers_dims[l], layers_dims[l - 1])) for l in range(1, L)]\n",
        "    self.v_db = [np.zeros((layers_dims[l], 1)) for l in range(1, L)]\n",
        "    self.s_dw = [np.zeros((layers_dims[l], layers_dims[l - 1])) for l in range(1, L)]\n",
        "    self.s_db = [np.zeros((layers_dims[l], 1)) for l in range(1, L)]\n",
        "    # Wight and bias\n",
        "    self.b = [np.zeros((layers_dims[l], 1)) for l in range(1, L)]\n",
        "    self.Activation = layers_activation\n",
        "    self.caches = [[] for _ in range(L - 1)]\n",
        "    self.dA = [[] for _ in range(L)]\n",
        "    self.dW = [[] for _ in range(L - 1)]\n",
        "    self.db = [[] for _ in range(L - 1)]\n",
        "    # Dropout\n",
        "    self.D = [[] for _ in range(L - 2)]\n",
        "\n",
        "    # For regularization: if there's no regularization, set lambda to 0\n",
        "    if self.regularization == None: self.lam = 0\n",
        "\n",
        "  def forward(self, X):\n",
        "\n",
        "    A = X\n",
        "\n",
        "    # for the forward propogation, make the predictions\n",
        "    for l in range(self.L):\n",
        "\n",
        "        A_prev = A \n",
        "        W = self.W[l]\n",
        "        b = self.b[l]\n",
        "        activation = self.Activation[l]\n",
        "\n",
        "        # the linear part\n",
        "        Z = np.dot(W, A_prev) + b\n",
        "\n",
        "        # a = activation(z)\n",
        "        if activation == \"sigmoid\":   \n",
        "          A = 1/(1 + np.exp(-Z))\n",
        "        elif activation == \"relu\": \n",
        "          A = np.maximum(0, Z)\n",
        "        elif activation == \"identity\":\n",
        "          A = Z\n",
        "        elif activation == \"tanh\":\n",
        "          A = np.tanh(Z)\n",
        "        elif activation == \"leaky_relu\":\n",
        "          A = np.fmax(0.01*Z, Z)\n",
        "        \n",
        "        # Implement Dropout in training models (not in computing MSE)\n",
        "        if l != self.L - 1 and self.task == \"train\":\n",
        "          A = (A * self.D[l]) / self.keep_prob\n",
        "  \n",
        "        # keep those in caches --> don't need to recompute it during back propogation\n",
        "        self.caches[l] = [A_prev, Z]\n",
        "\n",
        "        if l == self.L - 1:\n",
        "          Y_predict = A\n",
        "\n",
        "    return Y_predict\n",
        "  \n",
        " \n",
        "  def backward(self, Y_predict, Y):\n",
        "  \n",
        "    m = Y_predict.shape[1]\n",
        "\n",
        "    # initialize the back prop\n",
        "    self.dA[-1] = (Y_predict - Y)\n",
        "    \n",
        "    for l in range(self.L - 1, -1, -1):\n",
        "        \n",
        "        dA = self.dA[l + 1]\n",
        "\n",
        "        A_prev = self.caches[l][0]\n",
        "        Z = self.caches[l][1]\n",
        "        W = self.W[l]\n",
        "        b = self.b[l]\n",
        "        activation = self.Activation[l]\n",
        "\n",
        "        if activation == \"relu\":   \n",
        "          dA[Z <= 0] = 0\n",
        "          dZ = dA\n",
        "        elif activation == \"sigmoid\":\n",
        "          dZ = dA * (1/(1 + np.exp(-Z))) * (1 - (1/(1 + np.exp(-Z))))     \n",
        "        elif activation == \"tanh\":\n",
        "          dZ = dA * ((1 - np.tanh(Z)) ** 2)\n",
        "        elif activation == \"identity\":\n",
        "          dZ = dA\n",
        "        elif activation == \"leaky_relu\":\n",
        "          dA[Z <= 0] = 0.01\n",
        "          dZ = dA\n",
        "\n",
        "        # regularization is implemented\n",
        "        self.dW[l] = 1/m * np.dot(dZ, A_prev.T) + (self.lam / m) * W\n",
        "        self.db[l] = 1/m * np.sum(dZ, axis = 1, keepdims = True)\n",
        "        self.dA[l] = np.dot(W.T, dZ)\n",
        "        \n",
        "        # Dropout\n",
        "        if l != 0:\n",
        "          self.dA[l] = (self.dA[l] * self.D[l - 1]) / self.keep_prob\n",
        "        \n",
        "    \n",
        "  def update_sgd(self):\n",
        "    \n",
        "    if self.decay: self.learning_rate *= 1 / (1 + self.decay_rate * math.floor(self.epochs / self.interval))\n",
        "\n",
        "    for l in range(self.L):\n",
        "        self.W[l] = self.W[l] - self.learning_rate * self.dW[l]\n",
        "        self.b[l] = self.b[l] - self.learning_rate * self.db[l]\n",
        "  \n",
        "  def update_adam(self):\n",
        "\n",
        "    if self.decay: self.learning_rate *= 1 / (1 + self.decay_rate * math.floor(self.epochs / self.interval))\n",
        "\n",
        "    for l in range(self.L):\n",
        "      # The corrected version of v and s -- a combination of momentum and rmsprop\n",
        "      self.v_dw[l] = self.beta1 * self.v_dw[l] + (1 - self.beta1) * self.dW[l]\n",
        "      self.v_db[l] = self.beta1 * self.v_db[l] + (1 - self.beta1) * self.db[l]\n",
        "      v_dw_c = self.v_dw[l] / (1 - self.beta1 ** self.t)\n",
        "      v_db_c = self.v_db[l] / (1 - self.beta1 ** self.t)\n",
        "      self.s_dw[l] = self.beta2 * self.s_dw[l] + (1 - self.beta2) * np.square(self.dW[l])\n",
        "      self.s_db[l] = self.beta2 * self.s_db[l] + (1 - self.beta2) * np.square(self.db[l])\n",
        "      s_dw_c = self.s_dw[l] / (1 - self.beta2 ** self.t)\n",
        "      s_db_c = self.s_db[l] / (1 - self.beta2 ** self.t)\n",
        "\n",
        "      # Update w and b\n",
        "      self.W[l] = self.W[l] - self.learning_rate * v_dw_c / (np.sqrt(s_dw_c) + self.epsilon)\n",
        "      self.b[l] = self.b[l] - self.learning_rate * v_db_c / (np.sqrt(s_db_c) + self.epsilon)\n",
        "\n",
        "\n",
        "  def dropout(self, m):\n",
        "\n",
        "    for hidden_layer in range(self.L - 1):\n",
        "      self.D[hidden_layer] = np.random.rand(self.layers_dims[hidden_layer + 1], m) < self.keep_prob\n",
        "  \n",
        "  def fit(self, X, Y):\n",
        "\n",
        "    X = np.array(X).T\n",
        "    Y = np.array(Y).reshape(1, -1)\n",
        "\n",
        "    m = Y.shape[1]\n",
        "\n",
        "    if self.batch == None: self.batch = m\n",
        "    # Random pick the mini_batches\n",
        "    mini_batches = []\n",
        "    permutation = list(np.random.permutation(m))\n",
        "    shuffled_X = X[:, permutation]\n",
        "    shuffled_Y = Y[:, permutation].reshape((1, m))\n",
        "    count = math.floor(m / self.batch)\n",
        "    for k in range(count):\n",
        "        mini_batches.append((shuffled_X[:, k*self.batch : (k+1)*self.batch],shuffled_Y[:, k*self.batch: (k+1)*self.batch]))\n",
        "    if m % self.batch != 0:\n",
        "        mini_batches.append((shuffled_X[:, int(m/self.batch)*self.batch :],shuffled_Y[:, int(m/self.batch)*self.batch : ]))\n",
        "\n",
        "    for i in range(self.epochs):\n",
        "      for mini_batch in mini_batches:\n",
        "        (minibatch_X, minibatch_Y) = mini_batch\n",
        "        self.dropout(minibatch_Y.shape[1])\n",
        "        Y_predict = self.forward(minibatch_X)\n",
        "        self.backward(Y_predict, minibatch_Y)\n",
        "\n",
        "        # sgd and adam update methods  \n",
        "        if self.solver == 'sgd':\n",
        "          self.update_sgd()\n",
        "        elif self.solver == 'adam':\n",
        "          self.t += 1\n",
        "          self.update_adam()\n",
        "  \n",
        "  def MSE(self, X, Y):\n",
        "\n",
        "    # Do not use dropout during the calculation of error\n",
        "    self.task = 'test'\n",
        "\n",
        "    X = np.array(X).T\n",
        "    Y = np.array(Y).reshape(1, -1)\n",
        "    Y_predict = self.forward(X)\n",
        "\n",
        "    self.task = 'train'\n",
        "\n",
        "    return np.mean((Y_predict - Y) ** 2)  \n",
        "  "
      ],
      "metadata": {
        "id": "wxLLHv-vh9uM"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "boston = pd.read_csv(\"http://43.143.180.76/data/boston.csv\").sample(frac=1.0).reset_index(drop=True)\n",
        "n = len(boston) * 0.7\n",
        "columns = list(boston.columns)\n",
        "\n",
        "X_train = np.array(boston.loc[:n][columns[:-1]])\n",
        "Y_train = np.array(boston.loc[:n][columns[-1]])\n",
        "X_test = np.array(boston.loc[n:][columns[:-1]])\n",
        "Y_test = np.array(boston.loc[n:][columns[-1]])\n",
        "\n",
        "print(\"X.shape:\", X_train.shape)\n",
        "print(\"Y.shape\", Y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_N0yN9jlfDPs",
        "outputId": "35c368b4-8fd6-4658-8975-cd7c1b509f6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X.shape: (355, 13)\n",
            "Y.shape (355,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = DNNR()\n",
        "model.fit(X_train,Y_train)\n",
        "mse_train = model.MSE(X_train,Y_train)\n",
        "mse_test = model.MSE(X_test,Y_test)\n",
        "print(mse_train)\n",
        "print(mse_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MYdPiKEfQj8",
        "outputId": "52eb4e1a-e7be-44ba-d3b3-f0c450b22278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.522385101882724\n",
            "8.71071611879156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GridSearch:\n",
        "  def __init__(self, learning_rate = [0.001,0.01,0.1], epochs = [100,3000,5000], layers_dims = [13, 10, 5, 1], \n",
        "               layers_activation = [['relu','relu','leaky_relu'],['relu','relu','identity'],['relu','tanh','identity']], weight_init = ['Xavier','He'],\n",
        "               keep_prob = [0.9,0.8,0.5], regularization = [None,'l2']):\n",
        "\n",
        "    self.learning_rate = learning_rate\n",
        "    self.epochs = epochs\n",
        "    self.layers_dims = layers_dims # keep the structure the same for simplicity\n",
        "    self.layers_activation = layers_activation\n",
        "    self.weight_init = weight_init\n",
        "    self.keep_prob = keep_prob\n",
        "    self.regularization = regularization\n",
        "  \n",
        "  def tunning(self, X_train, Y_train, X_test, Y_test):\n",
        "    \n",
        "    index = []\n",
        "    mse_train = []\n",
        "    mse_test = []\n",
        "\n",
        "    mse_test_best = math.inf\n",
        "    best_model = None\n",
        "\n",
        "    for i in self.learning_rate:\n",
        "      for j in self.epochs:\n",
        "        for k in self.layers_activation:\n",
        "          for l in self.weight_init:\n",
        "            for m in self.keep_prob:\n",
        "              for n in self.regularization:\n",
        "                \n",
        "                description = \"learning_rate \"+str(i)+\" epochs \"+str(j)+\" layers_activation \"+str(k)+\" weight_init \"+str(l)+\" keep_prob \"+str(m)+\" regularization \"+str(n)\n",
        "                index.append(description)\n",
        "                model = DNNR(learning_rate = i, epochs = j, layers_dims = self.layers_dims, layers_activation = k, weight_init = l, keep_prob = m, regularization = n)\n",
        "                model.fit(X_train,Y_train)\n",
        "                mse_train_temp = model.MSE(X_train,Y_train)\n",
        "                mse_test_temp = model.MSE(X_test,Y_test)\n",
        "                mse_train.append(mse_train_temp)\n",
        "                mse_test.append(mse_test_temp)\n",
        "\n",
        "                if mse_test_temp < mse_test_best:\n",
        "                  mse_test_best = mse_test_temp\n",
        "                  best_model = description\n",
        "\n",
        "    Result=pd.DataFrame({'MSE_Train':mse_train,'MSE_Test':mse_test},index=index)\n",
        "\n",
        "    return Result, mse_test_best, best_model\n",
        "        "
      ],
      "metadata": {
        "id": "AeN0u_DDPdKi"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid = GridSearch()\n",
        "Result, mse_test_best, best_model = grid.tunning(X_train, Y_train, X_test, Y_test)\n",
        "print(Result)\n",
        "print(best_model,\" \",mse_test_best)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8E6Pn7bTTks",
        "outputId": "07b60a42-1139-4335-bae1-84b6a5f773dc"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-42-c4c10cb4893d>:126: RuntimeWarning: invalid value encountered in multiply\n",
            "  A = (A * self.D[l]) / self.keep_prob\n",
            "<ipython-input-42-c4c10cb4893d>:174: RuntimeWarning: invalid value encountered in multiply\n",
            "  self.dA[l] = (self.dA[l] * self.D[l - 1]) / self.keep_prob\n",
            "<ipython-input-42-c4c10cb4893d>:168: RuntimeWarning: invalid value encountered in multiply\n",
            "  self.dW[l] = 1/m * np.dot(dZ, A_prev.T) + (self.lam / m) * W\n",
            "/usr/local/lib/python3.9/dist-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/usr/local/lib/python3.9/dist-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                     MSE_Train    MSE_Test\n",
            "learning_rate 0.001 epochs 100 layers_activatio...   78.598097  100.418919\n",
            "learning_rate 0.001 epochs 100 layers_activatio...   71.650800   92.542594\n",
            "learning_rate 0.001 epochs 100 layers_activatio...   89.722414  113.843922\n",
            "learning_rate 0.001 epochs 100 layers_activatio...   99.994121  124.300246\n",
            "learning_rate 0.001 epochs 100 layers_activatio...  198.887478  234.320988\n",
            "...                                                        ...         ...\n",
            "learning_rate 0.1 epochs 5000 layers_activation...   77.286894   99.676442\n",
            "learning_rate 0.1 epochs 5000 layers_activation...   78.099184   99.477847\n",
            "learning_rate 0.1 epochs 5000 layers_activation...   76.394006   94.896369\n",
            "learning_rate 0.1 epochs 5000 layers_activation...   78.100326   99.499569\n",
            "learning_rate 0.1 epochs 5000 layers_activation...   78.124565   99.245163\n",
            "\n",
            "[324 rows x 2 columns]\n",
            "learning_rate 0.01 epochs 3000 layers_activation ['relu', 'relu', 'leaky_relu'] weight_init Xavier keep_prob 0.9 regularization None   9.968250235943838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Result = Result.sort_values(['MSE_Test'])\n",
        "print(Result[:10])\n",
        "Result.to_csv(\"Result.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egRwns8WWkTC",
        "outputId": "cb453cee-fbee-4099-e69c-1cbd314f2608"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                    MSE_Train   MSE_Test\n",
            "learning_rate 0.01 epochs 3000 layers_activatio...   8.213663   9.968250\n",
            "learning_rate 0.01 epochs 5000 layers_activatio...   8.221687  10.242608\n",
            "learning_rate 0.01 epochs 5000 layers_activatio...   7.794007  10.789662\n",
            "learning_rate 0.01 epochs 5000 layers_activatio...   9.132959  10.961063\n",
            "learning_rate 0.01 epochs 3000 layers_activatio...   7.960682  11.260889\n",
            "learning_rate 0.01 epochs 5000 layers_activatio...  10.115284  11.312658\n",
            "learning_rate 0.01 epochs 5000 layers_activatio...  11.118562  13.376822\n",
            "learning_rate 0.01 epochs 3000 layers_activatio...   9.825027  13.601492\n",
            "learning_rate 0.01 epochs 5000 layers_activatio...  10.077228  14.189613\n",
            "learning_rate 0.01 epochs 5000 layers_activatio...  11.239146  15.411829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MLP = MLPRegressor(max_iter = 3000, learning_rate_init=0.01)\n",
        "MLP = MLP.fit(X_train,Y_train)\n",
        "MLP_Y_train_predict = MLP.predict(X_train)\n",
        "MLP_Y_test_predict = MLP.predict(X_test)\n",
        "mse_mlp_train = mean_squared_error(Y_train,MLP_Y_train_predict)\n",
        "mse_mlp_test = mean_squared_error(Y_test,MLP_Y_test_predict)\n",
        "print(mse_mlp_train)\n",
        "print(mse_mlp_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDxsTYmxJWJn",
        "outputId": "cc6ca35e-c845-4664-b555-0cc28cf6078c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.069845925215745\n",
            "12.372685636773847\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Compared DNN with the previous Linear Regression model, the result is better obviously. Previously, with iter = 3000 and learning_rate = 0.01, the result is about 20.\n",
        "- Compared DNN with MLP, the results are kind of similar if I run the result multiple times. Both MSE are around 10."
      ],
      "metadata": {
        "id": "IP2TXw9Hg3s7"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}